import os
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import OpenAI


# Carregar vari√°veis do ambiente
load_dotenv()


class PEEAgent:
    def __init__(self, api_key):
        """Inicializa o agente carregando os embeddings dispon√≠veis dentro de PEE (PEX, PDC, PAF, PLX e Geral)."""
        
        # üìå Diret√≥rio base dos embeddings
        base_path = "embeddings/PEE/"
        
        # üìå Lista de subpastas esperadas
        subfolders = ["PEX", "PDC", "PAF", "PLX", "geral"]
        
        # üìå Filtrar apenas as pastas que realmente cont√™m embeddings
        valid_paths = []
        for folder in subfolders:
            folder_path = os.path.join(base_path, folder)
            if os.path.exists(folder_path) and os.path.isfile(os.path.join(folder_path, "index.faiss")):
                valid_paths.append(folder_path)

        # üìå Se n√£o houver embeddings dispon√≠veis, lan√ßar erro
        if not valid_paths:
            raise ValueError("‚ùå Nenhuma pasta de embeddings v√°lida encontrada para PEE. Verifique se os embeddings foram gerados.")

        # üìå Carregar todos os embeddings dispon√≠veis
        self.vector_stores = []
        for path in valid_paths:
            try:
                store = FAISS.load_local(path, OpenAIEmbeddings(openai_api_key=api_key), allow_dangerous_deserialization=True)
                self.vector_stores.append(store)
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao carregar embeddings de {path}: {e}")

        # üìå Criar um √∫nico retriever que busca em todas as bases carregadas
        if not self.vector_stores:
            raise ValueError("‚ùå N√£o foi poss√≠vel carregar nenhum embedding v√°lido para PEE.")
        
        self.retrievers = [store.as_retriever() for store in self.vector_stores]
        self.llm = OpenAI(openai_api_key=api_key)

    def answer_question(self, question: str, chat_history: list = None) -> str:
        """
        Gera uma resposta para a pergunta baseada no hist√≥rico do chat e nos materiais dispon√≠veis dos cursos PEX, PDC, PAF, PLX e Geral.
        O fluxo √©:
          1. Verifica√ß√£o do hist√≥rico do chat (se existir).
          2. Busca nos embeddings dispon√≠veis.
          3. Gera√ß√£o de uma resposta inicial com o contexto.
          4. Refinamento da resposta para torn√°-la mais clara e did√°tica.
        """
        if not chat_history or not isinstance(chat_history, list):
            chat_history = []

        # üîπ Formatar hist√≥rico do chat
        history_context = "\n".join([msg["message"] for msg in chat_history if isinstance(msg, dict) and "message" in msg]) if chat_history else "Sem hist√≥rico dispon√≠vel."

        # üîπ Buscar informa√ß√µes relevantes nos embeddings
        relevant_docs = []
        for retriever in self.retrievers:
            relevant_docs.extend(retriever.invoke(question))

        if not relevant_docs:
            return "N√£o encontrei informa√ß√µes nos materiais dos cursos PEX, PDC, PAF e PLX sobre isso."

        # üîπ Converter documentos relevantes para string e evitar conflitos no f-string
        docs_str = str(relevant_docs).replace("{", "{{").replace("}", "}}")

        # üîπ Criar prompt inicial com hist√≥rico + contexto + documentos
        formatted_input = f"""
Hist√≥rico do chat:
{history_context}

Baseando-se nos materiais dispon√≠veis dos cursos PEX, PDC, PAF, PLX e Geral, responda com clareza e detalhes a seguinte pergunta:
{question}

Documentos relevantes:
{docs_str}
"""
        # üîπ Gerar resposta inicial com o LLM
        initial_response = self.llm.invoke(formatted_input)

        # üîπ Criar prompt de refinamento para melhorar a resposta inicial
        refine_prompt = f"""
Aqui est√° uma resposta inicial gerada para a pergunta "{question}":
---
{initial_response}
---
Agora, reescreva a resposta para que fique mais clara, objetiva e did√°tica, removendo redund√¢ncias e melhorando a legibilidade.
"""
        refined_response = self.llm.invoke(refine_prompt)

        return refined_response
