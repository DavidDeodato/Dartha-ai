import os
from dotenv import load_dotenv
from langchain_openai import OpenAI

# Carregar vari√°veis do ambiente
load_dotenv()
OPENAI_API_KEY = os.getenv("OPEN_AI_API_KEY")

class LegalAI:
    def __init__(self):
        """Inicializa o modelo jur√≠dico, focado em consultas sobre direito e legisla√ß√£o."""
        self.llm = OpenAI(openai_api_key=OPENAI_API_KEY)

    def answer_question(self, question: str, chat_history: list = None) -> str:
        """
        Gera uma resposta para quest√µes jur√≠dicas usando um prompt estruturado.
        O fluxo √©:
          1. Recebe a pergunta do usu√°rio e o hist√≥rico do chat.
          2. Formata a entrada de acordo com a necessidade jur√≠dica.
          3. Gera uma resposta baseada na legisla√ß√£o vigente.
          4. Refinamento da resposta para maior clareza e precis√£o.
        """
        # Verifica se h√° hist√≥rico v√°lido
        if not chat_history or not isinstance(chat_history, list):
            chat_history = []

        # Formatar o hist√≥rico de chat (mas apenas se for passado no endpoint)
        history_context = "\n".join(
            [msg["message"] for msg in chat_history if isinstance(msg, dict) and "message" in msg]
        ) if chat_history else ""

        # Criando um prompt estruturado para perguntas jur√≠dicas
        legal_prompt = (
            "Voc√™ √© um assistente jur√≠dico especializado em leis brasileiras e internacionais. "
            "Seu objetivo √© fornecer respostas claras, objetivas e baseadas na legisla√ß√£o vigente.\n\n"
            "‚öñÔ∏è Contexto:\n"
        )
        
        if history_context:
            legal_prompt += f"Hist√≥rico da conversa:\n{history_context}\n\n"

        legal_prompt += f'O usu√°rio fez a seguinte pergunta:\n"{question}"\n\n'
        legal_prompt += (
            "üìå Diretrizes para sua resposta:\n"
            "- Forne√ßa uma explica√ß√£o clara e objetiva.\n"
            "- Cite a base legal relevante, se aplic√°vel.\n"
            "- Caso a quest√£o seja muito espec√≠fica ou dependa de an√°lise aprofundada, informe que a resposta pode variar conforme o caso.\n\n"
            "Agora, elabore uma resposta detalhada:"
        )

        # Gera resposta inicial
        initial_response = self.llm.invoke(legal_prompt)

        # Refinamento da resposta
        refine_prompt = f"""
Aqui est√° a resposta inicial para a pergunta jur√≠dica:
---
{initial_response}
---
Agora, refine a resposta para que fique ainda mais clara, objetiva e alinhada com as leis e normas vigentes.
"""
        refined_response = self.llm.invoke(refine_prompt)

        return refined_response
