from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import OpenAI
import os
from dotenv import load_dotenv

# Carregar vari√°veis do ambiente
load_dotenv()
OPENAI_API_KEY = os.getenv("OPEN_AI_API_KEY")

class GIX5Agent:
    def __init__(self):
        """Inicializa o agente carregando os embeddings do curso GIX5."""
        self.vector_store = FAISS.load_local(
            "embeddings/GIX5",
            OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY),
            allow_dangerous_deserialization=True  # Permite carregamento seguro do FAISS
        )
        self.retriever = self.vector_store.as_retriever()
        self.llm = OpenAI(openai_api_key=OPENAI_API_KEY)

    def answer_question(self, question: str, chat_history: list) -> str:
        """Pipeline de resposta baseado no hist√≥rico do chat."""

        # ‚úÖ Transforma o hist√≥rico de mensagens em texto leg√≠vel
        history_context = "\n".join(
            [msg["message"] for msg in chat_history if isinstance(msg, dict) and "message" in msg]
        ) if chat_history else "Sem hist√≥rico dispon√≠vel."

        # üîπ Passo 1: Buscar informa√ß√µes relevantes nos embeddings
        relevant_docs = self.retriever.invoke(question)

        if not relevant_docs:
            return "N√£o encontrei informa√ß√µes no material do curso sobre isso."

        # üîπ Passo 2: Gerar uma resposta baseada no material e no hist√≥rico
        formatted_input = f"""
        Hist√≥rico do chat:
        {history_context}

        Baseando-se no material do curso GIX5, responda com clareza e detalhes:
        {question}
        """
        initial_response = self.llm.invoke(formatted_input)

        # üîπ Passo 3: Refinar a resposta para torn√°-la mais objetiva e compreens√≠vel
        refine_prompt = f"""
        Aqui est√° uma resposta inicial gerada sobre "{question}":
        ---
        {initial_response}
        ---
        Agora, reescreva a resposta para que fique mais clara, objetiva e did√°tica, removendo redund√¢ncias e melhorando a legibilidade.
        """
        refined_response = self.llm.invoke(refine_prompt)

        return refined_response