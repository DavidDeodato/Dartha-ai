import os
from dotenv import load_dotenv
from langchain_openai import OpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings


# Carregar vari√°veis do ambiente
load_dotenv()

class Marketing:
    def __init__(self, api_key):
        """Inicializa o modelo 'Marketing' com o prompt de sistema definido."""

        # Definir API Key
        self.api_key = api_key

        # Configurar LLM principal
        self.llm = OpenAI(openai_api_key=api_key)

        # Vari√°vel para ativar/desativar manualmente o RAG
        self.usar_rag = True  

        # Caminho dos embeddings
        self.embeddings_path = "embeddings/MARKETING"

        # Verifica se a pasta de embeddings est√° vazia ou n√£o existe
        if not os.path.exists(self.embeddings_path) or not os.listdir(self.embeddings_path):
            print(f"‚ö†Ô∏è A pasta de embeddings '{self.embeddings_path}' est√° vazia ou n√£o existe. Desativando RAG automaticamente.")
            self.usar_rag = False

        # Se usar_rag for True, inicializa FAISS
        if self.usar_rag:
            try:
                self.vector_store = FAISS.load_local(
                    self.embeddings_path,
                    OpenAIEmbeddings(openai_api_key=api_key),
                    allow_dangerous_deserialization=True
                )
                self.retriever = self.vector_store.as_retriever()
            except Exception as e:
                print(f"‚ùå Erro ao carregar os embeddings: {e}")
                self.usar_rag = False

    def answer_question(self, question: str, chat_history: list = None) -> str:
        """Gera uma resposta baseada no hist√≥rico do chat e no conhecimento embutido (com ou sem RAG)."""

        # 1. Verifica se h√° hist√≥rico v√°lido
        if not chat_history or not isinstance(chat_history, list):
            chat_history = []

        history_context = "\n".join(
            [msg["message"] for msg in chat_history if isinstance(msg, dict) and "message" in msg]
        ) if chat_history else "Sem hist√≥rico dispon√≠vel."

        # 2. Se RAG estiver ativado, buscar documentos relevantes
        retrieved_docs = ""
        if self.usar_rag:
            try:
                relevant_docs = self.retriever.invoke(question)
                if relevant_docs:
                    retrieved_docs = "\n".join([doc.page_content for doc in relevant_docs])
            except Exception as e:
                print(f"‚ö†Ô∏è Erro ao recuperar documentos do RAG: {e}")
                self.usar_rag = False

        # 3. Criar o prompt inicial corretamente formatado
        formatted_input = f"""
üîπ Marketing Assistente:
Hist√≥rico do chat:
{history_context}

Pergunta do usu√°rio:
{question}
"""

        # Adiciona os documentos relevantes apenas se RAG estiver ativado e se houver algo recuperado
        if self.usar_rag and retrieved_docs:
            formatted_input += f"\n\nDocumentos relevantes:\n{retrieved_docs}"

        # 4. Gera√ß√£o da resposta inicial
        initial_response = self.llm.invoke(formatted_input)

        # 5. Refinamento da resposta
        refine_prompt = f"""
Aqui est√° a resposta inicial:
---
{initial_response}
---
Agora, refine a resposta para que fique ainda mais clara, objetiva e did√°tica.
"""
        refined_response = self.llm.invoke(refine_prompt)

        return refined_response
